# 실험 계획서

## 프로젝트 개요
- 데이터: train1 (Caucasian male, 2,882 identities), train2 (Korean mixed gender, 949 identities)
- 목표: Face Recognition 모델 성능 향상 (EER 감소)
- 제약사항: 모델 파라미터 11.5M 이하 (FC layer 제외), 외부 데이터/모델 사용 불가

---

## 1. 모델 실험 계획

### 1.1 Baseline 모델
- **ResNet18** (현재 구현됨)
  - 예상 파라미터: ~11.4M
  - 기본 실험으로 성능 기준선 확보

### 1.2 모델 변형 실험 (11.5M 파라미터 제한 내)

**주의**: torchvision의 기본 모델들은 채널 수를 직접 조정하는 파라미터가 없습니다. 채널 수를 조정하려면 커스텀 모델 구현이 필요합니다.

- **ResNet34** (커스텀 구현 또는 torchvision 사용)
  - ResNet18보다 깊은 네트워크
  - torchvision.models.resnet34 사용 가능 (파라미터 수 확인 필요)
  - 파라미터가 11.5M 초과 시 채널 수를 줄인 커스텀 버전 구현 필요

- **ResNet18 변형 (커스텀 구현)**
  - **Width Multiplier 적용**: 각 블록의 채널 수를 일정 비율로 조정 (예: 0.75x, 0.5x)
    - 예: 기본 채널 [64, 128, 256, 512] → [48, 96, 192, 384] (0.75x)
    - 파라미터 수를 크게 줄일 수 있음
  - **Bottleneck 구조 변경**: BasicBlock vs BottleneckBlock
  - **Attention 메커니즘 추가**: SE-Net, CBAM 등 (파라미터 증가 고려)
    - SE-Net: 채널별 attention, 파라미터 증가 적음
    - CBAM: 채널 + 공간 attention, 파라미터 증가 중간

- **EfficientNet-B0** (커스텀 구현 필요)
  - torchvision의 EfficientNet은 width multiplier를 지원하지만, 기본 B0가 11.5M 초과 가능
  - 커스텀으로 구현하여 width multiplier를 0.8x, 0.7x 등으로 조정
  - 또는 EfficientNet-Lite 버전 사용 고려

- **MobileNetV2** (커스텀 구현 필요)
  - torchvision의 MobileNetV2는 width multiplier 지원
  - width_mult 파라미터로 채널 수 조정 가능 (예: width_mult=0.75)
  - 하지만 기본 MobileNetV2도 파라미터 수 확인 필요

- **TinyViT-11M** (timm 라이브러리 사용)
  - 경량 Vision Transformer 모델
  - 약 10.55M 파라미터 (FC layer 제외)로 11.5M 제한 만족
  - timm 라이브러리 필요 (pip install timm)
  - Transformer 아키텍처 사용 (프로젝트 규칙 해석 필요 - TA 확인 권장)

- **간단한 접근 방법**:
  - ResNet18을 커스텀으로 구현하여 width multiplier 추가
  - 또는 torchvision 모델을 로드한 후 첫 번째 conv 레이어의 출력 채널 수를 수정
  - 하지만 이는 모델 구조가 복잡해질 수 있음

- tiny_vit_11m_224
  - 전체 파라미터: 10,777,860 (10.78M)
  - Head/Classifier: 230,784
  - Backbone (FC 제외): 10,547,076 (10.55M)

### 1.3 모델 실험 우선순위
1. **ResNet18** (baseline) - torchvision 사용, 파라미터 수 확인
2. **ResNet18 + SE-Net Attention** - 커스텀 구현, 파라미터 수 증가 적음
3. **ResNet34** - torchvision 사용, 파라미터 수 확인 후 필요시 커스텀 버전
4. **EfficientNet-B0** - torchvision 사용, 경량 모델 (약 4.01M)
5. **TinyViT-11M** - timm 라이브러리 사용, Vision Transformer (약 10.55M)
6. **ResNet18 (Width 0.75x)** - 커스텀 구현, 파라미터 수 감소
7. **MobileNetV2** - torchvision 사용 (width_mult 조정 가능)

**구현 난이도**:
- 쉬움: torchvision/timm 모델 그대로 사용 (ResNet18, ResNet34, EfficientNet-B0, TinyViT-11M, MobileNetV2)
- 중간: SE-Net 같은 attention 추가 (ResNet18 기반)
- 어려움: 커스텀 ResNet 구현 (width multiplier 포함)

**권장 접근**:
1. 먼저 torchvision/timm의 기본 모델들(ResNet18, ResNet34, EfficientNet-B0, TinyViT-11M, MobileNetV2)을 사용하여 파라미터 수 확인
2. 11.5M 초과 시, 커스텀 구현으로 채널 수 조정
3. 또는 SE-Net 같은 경량 attention 추가로 성능 향상 시도
4. TinyViT-11M은 Transformer 아키텍처이므로 프로젝트 규칙 해석 확인 필요 (TA 문의 권장)

---

## 2. Loss Function 실험 계획

### 2.1 현재 구현된 Loss
- **Softmax Loss** (baseline)
- **Triplet Loss** (현재 구현됨, random negative mining)

### 2.2 추가 Loss Function 실험 (도메인 적응 관점)

- **ArcFace Loss** ⭐⭐⭐⭐⭐ (최우선 추천)
  - Angular margin 기반, face recognition에서 매우 효과적
  - 도메인 간 일반화 성능 우수 (angular space에서 학습)
  - Scale, margin 하이퍼파라미터 튜닝 필요
  - 일반적으로 scale=64, margin=0.5 사용

- **Triplet Loss (Hard Negative Mining)** ⭐⭐⭐⭐
  - 현재는 random negative mining 사용
  - Hard negative mining으로 어려운 샘플에 집중
  - 도메인 간 일반화 성능 향상

- **Softmax + ArcFace (Combined Loss)** ⭐⭐⭐⭐⭐
  - 분류 성능 + Angular margin
  - 도메인 적응에 매우 효과적
  - 가중치 조정 실험 (예: 0.7:0.3, 0.5:0.5)

### 2.3 Loss Function 실험 우선순위

1. **Softmax** (baseline)
2. **Triplet Loss** (baseline, random negative mining)
3. **ArcFace Loss** ⭐⭐⭐⭐⭐
   - 도메인 적응에 가장 효과적
   - scale=64, margin=0.5부터 시작
4. **Triplet Loss (Hard negative mining)**
   - 어려운 샘플에 집중하여 일반화 성능 향상
5. **Softmax + ArcFace (Combined)** ⭐⭐⭐⭐⭐
   - 가장 강력한 조합 예상
   - 가중치: 0.7:0.3, 0.5:0.5 실험

---

## 3. 하이퍼파라미터 실험 계획

### 3.1 Learning Rate 실험
- **Baseline**: 0.001
- **실험 범위**:
  - 0.0001 (더 작은 LR, 안정적 학습)
  - 0.0005
  - 0.001 (baseline)
  - 0.005 (더 큰 LR, 빠른 수렴)
  - 0.01

- **Learning Rate Schedule**:
  - StepLR (현재 구현됨, lr_decay=0.90)
  - CosineAnnealingLR - Cosine 함수로 부드럽게 LR 감소
    - T_max: 전체 epoch 수
    - eta_min: 최소 learning rate (예: 0.0001)
  - SGDR (CosineAnnealingWarmRestarts) - 주기적으로 warm restart
    - T_0: 첫 번째 restart까지의 epoch 수 (예: 10)
    - T_mult: restart 주기 배수 (예: 2 -> 10, 20, 40, ...)
    - eta_min: 최소 learning rate (예: 0.0001)

### 3.2 Optimizer 실험
- **Adam** (현재 구현됨)
  - weight_decay: 0, 1e-4, 1e-3, 1e-2
  - beta1, beta2 조정

- **SGD** (현재 구현됨)
  - momentum: 0.9 (기본값)
  - weight_decay: 0, 1e-4, 1e-3, 1e-2
  - nesterov: True/False

- **AdamW**
  - Adam의 개선 버전, weight decay 분리

- **RAdam**
  - Rectified Adam, 초기 학습 안정화

### 3.3 Batch Size 실험
- **Baseline**: 100 (classes per batch)
- **실험 범위**:
  - 50 (더 작은 batch, 더 자주 업데이트)
  - 100 (baseline)
  - 150
  - 200 (더 큰 batch, 안정적 gradient)

### 3.4 Epochs & Early Stopping
- **Baseline**: 10 epochs
- **실험**:
  - 15 epochs (더 긴 학습)
  - 20 epochs
  - Early stopping (validation EER 기반)

### 3.5 Embedding Dimension (nOut)
- **Baseline**: 512
- **실험 범위**:
  - 256 (더 작은 embedding)
  - 512 (baseline)
  - 1024 (더 큰 embedding, 표현력 증가)

### 3.6 Loss-specific 하이퍼파라미터
- **Triplet Loss**:
  - margin: 0.1 (baseline), 0.2, 0.3, 0.5
  - hard_rank, hard_prob 조정

- **ArcFace/CosFace**:
  - scale: 30 (baseline), 64, 128
  - margin: 0.1 (baseline), 0.3, 0.5

### 3.7 하이퍼파라미터 실험 우선순위
1. Learning rate: 0.001 → 0.0005, 0.005
2. Learning rate scheduler: StepLR → CosineAnnealingLR
3. Optimizer: Adam → AdamW, SGD (momentum 조정)
4. Weight decay: 0 → 1e-4, 1e-3
5. Embedding dimension: 512 → 256, 1024
6. Batch size: 100 → 50, 150

---

## 4. Data Augmentation 실험 계획

### 4.1 현재 구현된 Augmentation
- Resize(256)
- RandomCrop(224)
- Normalize (ImageNet mean/std)

### 4.2 추가 Augmentation 기법
- **기하학적 변환**:
  - RandomHorizontalFlip (p=0.5)
  - RandomRotation (degrees: 5, 10, 15)
  - RandomAffine (translation, scale, shear)

- **색상 변환**:
  - ColorJitter (brightness, contrast, saturation, hue)
  - RandomGrayscale (p=0.1)
  - Normalize 다양화 (다른 mean/std)

- **노이즈/블러**:
  - GaussianBlur (kernel_size=3, sigma 범위)
  - RandomErasing (p=0.3, scale, ratio)

- **고급 Augmentation**:
  - MixUp (두 이미지 혼합)
  - CutMix (이미지 일부 교체)
  - AutoAugment (자동 augmentation 정책)

- **Face-specific Augmentation**:
  - Random occlusion (안경, 마스크 등 - 규칙에 따라 외부 객체 오버레이 가능)
  - Lighting variation (조명 변화 시뮬레이션)

### 4.3 Augmentation 조합 전략
- **전략 1**: 보수적 (기하학적 변환만)
  - RandomHorizontalFlip + RandomRotation(5)
  
- **전략 2**: 중간 (색상 + 기하학적)
  - RandomHorizontalFlip + ColorJitter + RandomRotation(10)
  
- **전략 3**: 공격적 (모든 기법)
  - RandomHorizontalFlip + ColorJitter + RandomRotation + GaussianBlur + RandomErasing

### 4.4 Data Augmentation 실험 우선순위
1. RandomHorizontalFlip 추가
2. ColorJitter 추가
3. RandomRotation 추가
4. RandomErasing 추가
5. MixUp/CutMix 실험

---

## 5. 실험 시나리오 및 실행 계획

### Phase 1: Baseline 확인 및 최적화
**목표**: Baseline 코드가 정상 작동하는지 확인하고 기본 성능 확보

1. **Baseline 실행** (train1, train2 각각)
   - 데이터: /mnt/home/ee40034/data/train1, train2
   - 모델: ResNet18
   - Loss: Softmax
   - Optimizer: Adam (lr=0.001)
   - Scheduler: StepLR (lr_decay=0.90)
   - Augmentation: 기본 (Resize, RandomCrop, Normalize) - Data Augmentation은 나중에
   - Epochs: 10
   - 목표: project_rule.txt의 baseline 성능과 유사한지 확인

### Phase 2: Loss Function 실험
**목표**: 가장 효과적인 loss function 찾기

2. **Triplet Loss 실험**
   - Baseline과 동일한 설정, loss만 변경
   - margin: 0.1, 0.2, 0.3

3. **ArcFace Loss 실험**
   - scale: 30, 64
   - margin: 0.3, 0.5

4. **Combined Loss 실험**
   - Softmax + Triplet (가중치: 0.7:0.3, 0.5:0.5)

### Phase 3: 하이퍼파라미터 튜닝
**목표**: Phase 2에서 최고 성능 loss에 대해 하이퍼파라미터 최적화

5. **Learning Rate 실험**
   - lr: 0.0005, 0.001, 0.005
   - Scheduler: StepLR, CosineAnnealingLR

6. **Optimizer 실험**
   - Adam, AdamW, SGD (momentum=0.9)
   - weight_decay: 0, 1e-4, 1e-3

7. **Batch Size & Embedding Dimension**
   - batch_size: 50, 100, 150
   - nOut: 256, 512, 1024

### Phase 4: 모델 실험
**목표**: 최적 설정에 대해 모델 아키텍처 변경

9. **ResNet18 + SE-Net Attention**
   - ResNet18에 SE-Net attention 추가
   - 커스텀 구현 필요 (파라미터 수 확인)

10. **ResNet34**
    - torchvision.models.resnet34 사용
    - 파라미터 수 확인, 11.5M 초과 시 커스텀 버전 고려

11. **MobileNetV2**
    - torchvision.models.mobilenet_v2 사용
    - width_mult 파라미터로 조정 가능
    - 파라미터 수 확인

12. **ResNet18 커스텀 (Width Multiplier)**
    - ResNet18을 커스텀으로 구현하여 width multiplier 추가
    - 채널 수를 0.75x, 0.5x로 조정하여 파라미터 수 제어

### Phase 5: Fine-tuning & 최종 실험
**목표**: 모든 최적 설정 결합 및 최종 성능 확보

9. **최적 조합 실험**
   - Phase 2-4에서 최고 성능 조합 결합
   - train1로 pre-training → train2로 fine-tuning
   - 또는 train2로만 학습

10. **최종 평가**
    - Validation set에서 최종 EER 측정
    - Test set 예측 및 결과 저장

### Phase 6: Data Augmentation 실험 (나중에 진행)
**목표**: 최적 설정에 augmentation 추가하여 추가 성능 향상

11. **Augmentation 조합 실험**
    - 전략 1: RandomHorizontalFlip + RandomRotation
    - 전략 2: 전략 1 + ColorJitter
    - 전략 3: 전략 2 + RandomErasing

**참고**: 현재는 기본 augmentation만 사용 (Resize, RandomCrop, Normalize)

---

## 6. 실험 기록 방법

각 실험마다 다음 정보 기록:
- 실험 번호 및 이름
- 모델, Loss, Optimizer, Scheduler
- 하이퍼파라미터 (lr, weight_decay, batch_size, nOut, margin, scale 등)
- Data Augmentation 설정
- Train1/Train2 사용 여부
- Epoch별 Loss 및 Validation EER
- 최종 Validation EER
- 모델 파라미터 수
- 특이사항 및 분석

---

## 7. 예상 실험 시간

- Baseline 확인: 2-3시간 (train1: ~10시간, train2: ~40분)
- Loss Function 실험: 5-6개 실험 × 2-3시간 = 10-18시간
- 하이퍼파라미터 튜닝: 10-15개 실험 × 2-3시간 = 20-45시간
- Data Augmentation: 3-5개 실험 × 2-3시간 = 6-15시간
- 모델 실험: 2-3개 실험 × 2-3시간 = 4-9시간
- Fine-tuning: 3-5시간

**총 예상 시간**: 약 50-100시간 (GPU 시간)

---

## 8. 주의사항

1. **파라미터 수 제한**: 모든 모델 변경 시 파라미터 수 확인 필수
2. **디스크 공간**: Checkpoint 파일 관리 (100GB 제한)
3. **실험 재현성**: Random seed 고정
4. **규칙 준수**: 외부 데이터/모델 사용 금지
5. **Validation 성능**: Test set는 ground truth 없으므로 validation EER로만 평가

---

## 9. 우선순위 높은 실험 (빠른 성능 향상 기대)

1. **ArcFace Loss** - Face recognition에서 매우 효과적
2. **Learning Rate Schedule 개선** - CosineAnnealingLR
3. **Data Augmentation 추가** - RandomHorizontalFlip, ColorJitter
4. **Hard Negative Mining** - Triplet Loss 개선
5. **Weight Decay 추가** - Overfitting 방지

---

## 10. 실험 결과 분석 포인트

- 어떤 loss function이 가장 효과적인가?
- Learning rate와 scheduler의 영향은?
- Data augmentation의 기여도는?
- Train1 vs Train2 학습 전략 차이
- 모델 아키텍처 변경의 효과
- 하이퍼파라미터 간 상호작용

